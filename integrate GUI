import sys
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"   # 允许 OpenMP 多库并存
import warnings

warnings.filterwarnings("ignore")

# ----------------- 第 1 部分：通用 & 深度学习依赖 -----------------
import numpy as np
from pathlib import Path

from PIL import Image

import torch
import torch.nn as nn
from torchvision import transforms
import timm

from skimage import exposure, util
from skimage.color import rgb2gray
from skimage.transform import resize
from skimage.measure import label, regionprops
from skimage.morphology import binary_opening, binary_closing, square

# ----------------- 第 2 部分：PyQt5 GUI 依赖 -----------------
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget,
    QLabel, QPushButton, QFileDialog,
    QVBoxLayout, QHBoxLayout, QMessageBox,
    QFrame, QSpacerItem, QSizePolicy, QStackedWidget
)
from PyQt5.QtGui import QPixmap, QFont, QPalette, QColor
from PyQt5.QtCore import Qt, pyqtSignal


# =======================================================
# 基本路径和常量
# =======================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

UNET_WEIGHTS_PATH = os.path.join(BASE_DIR, "384_unet.pth")
EFFICIENTNET_WEIGHTS_PATH = os.path.join(BASE_DIR, "efficientnet.pth")

WELCOME_IMAGE_PATH = os.path.join(BASE_DIR, "welcome.jpeg")

CLASS_NAMES = ['Non-Glaucoma', 'Glaucoma']
NORM_MEAN = [0.485, 0.456, 0.406]
NORM_STD = [0.229, 0.224, 0.225]

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[INFO] Using device: {DEVICE}")


# =======================================================
# 小工具：兼容 PyTorch 2.6 的安全加载函数
# =======================================================
def safe_torch_load(path, map_location=None):
    """
    优先使用 weights_only=False（解决 PyTorch 2.6 的默认更改问题），
    如果旧版本 torch 不支持该参数，则退回到普通 torch.load。
    """
    try:
        return torch.load(path, map_location=map_location, weights_only=False)
    except TypeError:
        # 老版本 torch 没有 weights_only 参数
        return torch.load(path, map_location=map_location)


# =======================================================
# 第 3 部分：UNet 分割模型
# =======================================================
class UNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=1, init_features=32):
        super(UNet, self).__init__()
        features = init_features
        self.encoder1 = self._block(in_channels, features, name="enc1")
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder2 = self._block(features, features * 2, name="enc2")
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder3 = self._block(features * 2, features * 4, name="enc3")
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder4 = self._block(features * 4, features * 8, name="enc4")
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.bottleneck = self._block(features * 8, features * 16, name="bottleneck")

        self.upconv4 = nn.ConvTranspose2d(
            features * 16, features * 8, kernel_size=2, stride=2
        )
        self.decoder4 = self._block((features * 8) * 2, features * 8, name="dec4")
        self.upconv3 = nn.ConvTranspose2d(
            features * 8, features * 4, kernel_size=2, stride=2
        )
        self.decoder3 = self._block((features * 4) * 2, features * 4, name="dec3")
        self.upconv2 = nn.ConvTranspose2d(
            features * 4, features * 2, kernel_size=2, stride=2
        )
        self.decoder2 = self._block((features * 2) * 2, features * 2, name="dec2")
        self.upconv1 = nn.ConvTranspose2d(
            features * 2, features, kernel_size=2, stride=2
        )
        self.decoder1 = self._block(features * 2, features, name="dec1")

        self.conv = nn.Conv2d(
            in_channels=features, out_channels=out_channels, kernel_size=1
        )
        self.sigmoid = nn.Sigmoid()

    def _block(self, in_channels, features, name):
        return nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=features,
                kernel_size=3,
                padding=1,
                bias=False,
            ),
            nn.BatchNorm2d(num_features=features),
            nn.ReLU(inplace=True),
            nn.Conv2d(
                in_channels=features,
                out_channels=features,
                kernel_size=3,
                padding=1,
                bias=False,
            ),
            nn.BatchNorm2d(num_features=features),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool1(enc1))
        enc3 = self.encoder3(self.pool2(enc2))
        enc4 = self.encoder4(self.pool3(enc3))

        bottleneck = self.bottleneck(self.pool4(enc4))

        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.decoder4(dec4)
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.decoder3(dec3)
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.decoder2(dec2)
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.decoder1(dec1)

        logits = self.conv(dec1)
        return self.sigmoid(logits)


# =======================================================
# 第 4 部分：一些图像处理函数
# =======================================================
def get_bounding_box(binary_mask):
    binary = binary_mask > 0
    if not np.any(binary):
        height, width = binary_mask.shape
        return 0, 0, width, height

    labeled_image = label(binary)
    regions = regionprops(labeled_image)
    largest_region = max(regions, key=lambda r: r.area)
    min_row, min_col, max_row, max_col = largest_region.bbox
    return min_col, min_row, max_col - min_col, max_row - min_row


def gamma_correct(img, gamma=0.4):
    img_float = util.img_as_float(img)
    corrected = exposure.adjust_gamma(img_float, gamma=gamma)
    return util.img_as_ubyte(corrected)


def enhance_contrast_clahe(image, clip_limit=0.02, kernel_size=8):
    img_float = util.img_as_float(image)
    enhanced = exposure.equalize_adapthist(
        img_float,
        kernel_size=kernel_size,
        clip_limit=clip_limit
    )
    return util.img_as_ubyte(enhanced)


def segment_optic_disc_with_unet(image_path, unet_model):
    pil_image = Image.open(image_path).convert('RGB')
    original_img = np.array(pil_image)

    img_tensor = util.img_as_float(original_img)
    img_tensor = resize(img_tensor, (384, 384), anti_aliasing=True)
    img_tensor = torch.FloatTensor(img_tensor).permute(2, 0, 1).unsqueeze(0)

    model_device = next(unet_model.parameters()).device
    img_tensor = img_tensor.to(model_device)

    with torch.no_grad():
        output = unet_model(img_tensor)
        mask_pred = output.squeeze().cpu().numpy()
        mask_pred = (mask_pred > 0.5).astype(np.uint8)

    mask_pred = resize(mask_pred, original_img.shape[:2], anti_aliasing=False, order=0)
    mask_pred = (mask_pred > 0.5).astype(np.uint8) * 255

    return mask_pred


def crop_fundus_region(original_image, threshold=10):
    if original_image.ndim == 2:
        original_image = np.stack([original_image] * 3, axis=-1)

    gray_image = util.img_as_ubyte(rgb2gray(original_image))

    if np.max(gray_image) <= threshold:
        foreground_mask = np.ones_like(gray_image, dtype=np.uint8) * 255
    else:
        foreground_mask = (gray_image > threshold).astype(np.uint8) * 255

    min_col, min_row, width, height = get_bounding_box(foreground_mask)

    margin = min(100, width // 4, height // 4)
    col_start = max(0, min_col + margin)
    row_start = max(0, min_row + margin)
    col_end = min(original_image.shape[1], min_col + width - margin)
    row_end = min(original_image.shape[0], min_row + height - margin)

    if col_end <= col_start or row_end <= row_start:
        col_start, row_start, col_end, row_end = 0, 0, original_image.shape[1], original_image.shape[0]

    cropped_rgb = original_image[row_start:row_end, col_start:col_end]
    return cropped_rgb


def refine_mask_with_morphology(binary_mask):
    binary = binary_mask > 0
    opened = binary_opening(binary, footprint=square(5))
    closed = binary_closing(opened, footprint=square(5))
    return (closed * 255).astype(np.uint8)


def crop_optic_disc_region(rgb_image, optic_disc_mask):
    min_col, min_row, width, height = get_bounding_box(optic_disc_mask)

    scale_x = rgb_image.shape[1] / optic_disc_mask.shape[1]
    scale_y = rgb_image.shape[0] / optic_disc_mask.shape[0]
    min_col = int(min_col * scale_x)
    width = int(width * scale_x)
    min_row = int(min_row * scale_y)
    height = int(height * scale_y)

    center_x = min_col + width // 2
    center_y = min_row + height // 2

    half_crop_size = 250  # 裁剪 500x500 区域
    col_start = max(0, center_x - half_crop_size)
    row_start = max(0, center_y - half_crop_size)
    col_end = min(rgb_image.shape[1], center_x + half_crop_size)
    row_end = min(rgb_image.shape[0], center_y + half_crop_size)

    if col_end <= col_start or row_end <= row_start:
        col_start, row_start = 0, 0
        col_end = min(rgb_image.shape[1], 500)
        row_end = min(rgb_image.shape[0], 500)

    optic_disc_crop = rgb_image[row_start:row_end, col_start:col_end]
    final_optic_disc_crop = util.img_as_ubyte(
        resize(optic_disc_crop, (384, 384), anti_aliasing=True)
    )
    return final_optic_disc_crop


def preprocess_fundus_image(image_path, unet_model, debug=False):
    """
    端到端预处理：输入原始眼底图，输出 384x384 视盘区域图像
    """
    pil_image = Image.open(image_path).convert("RGB")
    original_image = np.array(pil_image)

    cropped_rgb = crop_fundus_region(original_image, threshold=10)

    enhanced_gray = gamma_correct(rgb2gray(cropped_rgb), gamma=0.4)
    enhanced_gray = enhance_contrast_clahe(enhanced_gray)

    optic_disc_mask = segment_optic_disc_with_unet(image_path, unet_model)
    refined_mask = refine_mask_with_morphology(optic_disc_mask)

    enhanced_rgb = enhance_contrast_clahe(cropped_rgb)
    final_crop = crop_optic_disc_region(enhanced_rgb, refined_mask)

    if debug:
        return {
            "original_image": original_image,
            "cropped_fundus": cropped_rgb,
            "optic_disc_mask": refined_mask,
            "final_optic_disc_crop": final_crop
        }
    else:
        return final_crop


# =======================================================
# 第 5 部分：分类模型（EfficientNet-B3 + 全连接头）
# =======================================================
class CombinedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.cnn_model = timm.create_model(
            'efficientnet_b3',
            pretrained=False,
            num_classes=0
        )
        num_features = self.cnn_model.num_features

        self.cnn_projection = nn.Sequential(
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 30)
        )

        self.final_layers = nn.Sequential(
            nn.Linear(30, 8),
            nn.ReLU(),
            nn.Linear(8, 2)
        )

    def forward(self, x):
        x1 = self.cnn_model(x)
        x1 = self.cnn_projection(x1)
        x = self.final_layers(x1)
        return x


_unet_instance = None
_effnet_instance = None


def get_unet_model():
    global _unet_instance
    if _unet_instance is None:
        if not os.path.exists(UNET_WEIGHTS_PATH):
            raise FileNotFoundError(f"UNet 权重文件不存在: {UNET_WEIGHTS_PATH}")

        model = UNet(in_channels=3, out_channels=1).to(DEVICE)
        model.eval()

        # ✅ 使用安全加载函数，解决 PyTorch 2.6 weights_only 问题
        checkpoint = safe_torch_load(UNET_WEIGHTS_PATH, map_location=DEVICE)
        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
            state = checkpoint["model_state_dict"]
        else:
            state = checkpoint
        model.load_state_dict(state)

        _unet_instance = model
        print("[INFO] UNet 模型加载成功")

    return _unet_instance


def get_effnet_model():
    global _effnet_instance
    if _effnet_instance is None:
        if not os.path.exists(EFFICIENTNET_WEIGHTS_PATH):
            raise FileNotFoundError(f"EfficientNet 权重文件不存在: {EFFICIENTNET_WEIGHTS_PATH}")

        model = CombinedModel().to(DEVICE)
        model.eval()

        # ✅ 同样使用安全加载函数
        checkpoint = safe_torch_load(EFFICIENTNET_WEIGHTS_PATH, map_location=DEVICE)
        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
            state = checkpoint["model_state_dict"]
        else:
            state = checkpoint
        model.load_state_dict(state)

        _effnet_instance = model
        print("[INFO] EfficientNet 模型加载成功")

    return _effnet_instance


# =======================================================
# 第 6 部分：推理接口
# =======================================================
def predict_single_cropped_image_array(img_array: np.ndarray) -> dict:
    """
    输入：已经裁剪好的 384x384x3 numpy 数组
    输出：预测结果 dict
    """
    model = get_effnet_model()

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(NORM_MEAN, NORM_STD)
    ])

    pil_image = Image.fromarray(img_array)
    image_tensor = transform(pil_image).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        outputs = model(image_tensor)

    probabilities = torch.softmax(outputs, dim=1).cpu().squeeze().numpy()
    predicted_index = int(np.argmax(probabilities))

    result = {
        'predicted_class': CLASS_NAMES[predicted_index],
    }
    for i, class_name in enumerate(CLASS_NAMES):
        result[f'prob_{class_name}'] = float(probabilities[i])

    return result


def glaucoma_predict_pipeline(image_path: str) -> dict:
    """
    对外暴露的主函数：给 GUI 调用
    输入：原始眼底图路径
    输出：包含预测结果和概率的 dict
    """
    if not os.path.exists(image_path):
        return {'error': f"file not found: {image_path}"}

    unet_model = get_unet_model()
    cropped_img = preprocess_fundus_image(image_path, unet_model=unet_model, debug=False)

    cls_result = predict_single_cropped_image_array(cropped_img)
    cls_result['filename'] = Path(image_path).name

    return cls_result


# =======================================================
# 第 7 部分：GUI —— 欢迎页 + 分析页 + 主窗口
# =======================================================
class WelcomePage(QWidget):
    start_clicked = pyqtSignal()

    def __init__(self, parent=None):
        super().__init__(parent)
        self.bg_pixmap = QPixmap()
        if os.path.exists(WELCOME_IMAGE_PATH):
            self.bg_pixmap.load(WELCOME_IMAGE_PATH)
        self.init_ui()

    def init_ui(self):
        self.setMinimumSize(800, 500)

        root_layout = QVBoxLayout(self)
        root_layout.setContentsMargins(0, 0, 0, 0)
        root_layout.setSpacing(0)

        self.bg_label = QLabel()
        self.bg_label.setAlignment(Qt.AlignCenter)
        self.bg_label.setStyleSheet("background-color: #000000;")
        root_layout.addWidget(self.bg_label)

        overlay_layout = QVBoxLayout(self.bg_label)
        overlay_layout.setContentsMargins(24, 24, 24, 24)
        overlay_layout.setSpacing(10)

        overlay_layout.addStretch()

        bottom_row = QHBoxLayout()

        self.welcome_text = QLabel("Welcome to the\nGlaucoma Detector")
        self.welcome_text.setFont(QFont("Microsoft YaHei", 18, QFont.Bold))
        self.welcome_text.setStyleSheet("color: white;")
        bottom_row.addWidget(self.welcome_text)

        bottom_row.addStretch()

        self.start_button = QPushButton("Let’s get started")
        self.start_button.setMinimumHeight(40)
        self.start_button.setFont(QFont("Microsoft YaHei", 10, QFont.Bold))
        self.start_button.setStyleSheet(
            "QPushButton {"
            "  background-color: #4c6fff;"
            "  color: white;"
            "  border-radius: 20px;"
            "  padding: 6px 20px;"
            "}"
            "QPushButton:hover {"
            "  background-color: #3b59d4;"
            "}"
        )
        self.start_button.clicked.connect(self.start_clicked.emit)
        bottom_row.addWidget(self.start_button)

        overlay_layout.addLayout(bottom_row)
        self.update_background()

    def update_background(self):
        if not self.bg_pixmap.isNull():
            size = self.size()
            if size.width() > 0 and size.height() > 0:
                scaled = self.bg_pixmap.scaled(
                    size,
                    Qt.KeepAspectRatioByExpanding,
                    Qt.SmoothTransformation
                )
                self.bg_label.setPixmap(scaled)
                self.bg_label.setStyleSheet("")
        else:
            self.bg_label.setText(
                "Welcome to the Glaucoma Detector\n\n"
                "(Background image not found. "
                "Please place welcome.jpeg in the same folder.)"
            )
            self.bg_label.setStyleSheet(
                "color: white; background-color: #2d3436;"
            )

    def resizeEvent(self, event):
        super().resizeEvent(event)
        self.update_background()


class AnalyzePage(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.current_image_path = None
        self.original_pixmap = None
        self.init_ui()

    def init_ui(self):
        self.setMinimumSize(900, 600)

        palette = self.palette()
        palette.setColor(QPalette.Window, QColor("#f5f7fb"))
        self.setPalette(palette)
        self.setAutoFillBackground(True)

        top_label = QLabel(
            "Please select a retinal fundus image for inspection\n"
            "(This demo uses optic disc segmentation (UNet) + glaucoma classification (EfficientNet)"
        )
        top_label.setAlignment(Qt.AlignCenter)
        top_label.setStyleSheet("color: #555555;")
        top_label.setFont(QFont("Microsoft YaHei", 10))

        self.image_label = QLabel(
            "Drop area\n\nor click the 'Add image' button below"
        )
        self.image_label.setAlignment(Qt.AlignCenter)
        self.image_label.setWordWrap(True)
        self.image_label.setStyleSheet(
            "color: #777777;"
            "background-color: #ffffff;"
            "border-radius: 12px;"
            "border: 1px dashed #b2bec3;"
        )

        image_card_layout = QVBoxLayout()
        image_card_layout.addWidget(self.image_label)

        image_card = QFrame()
        image_card.setLayout(image_card_layout)
        image_card.setStyleSheet(
            "QFrame { background-color: #ffffff; border-radius: 12px; }"
        )

        self.result_title = QLabel("Analysis result")
        self.result_title.setFont(QFont("Microsoft YaHei", 12, QFont.Bold))

        self.result_label = QLabel("No image has been analyzed yet.")
        self.result_label.setAlignment(Qt.AlignTop | Qt.AlignLeft)
        self.result_label.setWordWrap(True)
        self.result_label.setStyleSheet(
            "color: black;"
            "background-color: #f8f9fc;"
            "border-radius: 8px;"
            "padding: 10px;"
        )

        self.hint_label = QLabel(
            "⚠ This software is for course work and educational demonstration only. "
            "It must NOT be used for real medical diagnosis or self-assessment "
            "of glaucoma."
        )
        self.hint_label.setWordWrap(True)
        self.hint_label.setStyleSheet("color: black;")

        right_layout = QVBoxLayout()
        right_layout.addWidget(self.result_title)
        right_layout.addSpacing(6)
        right_layout.addWidget(self.result_label)
        right_layout.addSpacing(10)
        right_layout.addWidget(self.hint_label)
        right_layout.addStretch()

        right_card = QFrame()
        right_card.setLayout(right_layout)
        right_card.setStyleSheet(
            "QFrame { background-color: #ffffff; border-radius: 12px; "
            "border: 1px solid #dde0e7; }"
        )

        middle_layout = QHBoxLayout()
        middle_layout.addWidget(image_card, stretch=3)
        middle_layout.addSpacing(12)
        middle_layout.addWidget(right_card, stretch=2)
        middle_layout.setContentsMargins(12, 12, 12, 12)

        self.load_button = QPushButton("Add image")
        self.load_button.setMinimumHeight(36)
        self.load_button.setStyleSheet(
            "QPushButton {"
            "  background-color: #ffffff;"
            "  border-radius: 8px;"
            "  border: 1px solid #b2bec3;"
            "  padding: 6px 16px;"
            "  color: #2d3436;"
            "}"
            "QPushButton:hover {"
            "  background-color: #ecf0f1;"
            "}"
        )
        self.load_button.clicked.connect(self.load_image)

        self.analyze_button = QPushButton("Run analysis")
        self.analyze_button.setMinimumHeight(36)
        self.analyze_button.setEnabled(False)
        self.analyze_button.setStyleSheet(
            "QPushButton {"
            "  background-color: #4c6fff;"
            "  border-radius: 8px;"
            "  border: none;"
            "  padding: 6px 18px;"
            "  color: white;"
            "  font-weight: bold;"
            "}"
            "QPushButton:hover {"
            "  background-color: #3b59d4;"
            "}"
            "QPushButton:disabled {"
            "  background-color: #b3c3ff;"
            "}"
        )
        self.analyze_button.clicked.connect(self.run_analysis)

        bottom_layout = QHBoxLayout()
        bottom_layout.addWidget(self.load_button)
        bottom_layout.addWidget(self.analyze_button)
        bottom_layout.addItem(
            QSpacerItem(40, 20, QSizePolicy.Expanding, QSizePolicy.Minimum)
        )
        bottom_layout.setContentsMargins(12, 0, 12, 12)

        root_layout = QVBoxLayout(self)
        root_layout.setContentsMargins(0, 0, 0, 0)
        root_layout.setSpacing(6)
        root_layout.addWidget(top_label)
        root_layout.addLayout(middle_layout)
        root_layout.addLayout(bottom_layout)

    def load_image(self):
        file_path, _ = QFileDialog.getOpenFileName(
            self,
            "Select eye image",
            "",
            "Image files (*.png *.jpg *.jpeg *.bmp *.tif *.tiff);;"
            "All files (*.*)"
        )

        if not file_path:
            return

        if not os.path.exists(file_path):
            QMessageBox.warning(
                self, "Error", "File does not exist. Please try again."
            )
            return

        pixmap = QPixmap(file_path)
        if pixmap.isNull():
            QMessageBox.warning(
                self, "Error",
                "Failed to load this image file.\n"
                "Please check that its format is supported."
            )
            return

        self.current_image_path = file_path
        self.original_pixmap = pixmap
        self.update_image_display()

        self.analyze_button.setEnabled(True)

        self.result_label.setText(
            f"Loaded image: {os.path.basename(file_path)}\n\n"
            "Click the \"Run analysis\" button on the right."
        )
        self.result_label.setStyleSheet(
            "color: black;"
            "background-color: #f8f9fc;"
            "border-radius: 8px;"
            "padding: 10px;"
        )

    def update_image_display(self):
        if self.original_pixmap is None:
            return

        target_size = self.image_label.size()
        if target_size.width() <= 0 or target_size.height() <= 0:
            return

        scaled_pixmap = self.original_pixmap.scaled(
            target_size,
            Qt.KeepAspectRatio,
            Qt.SmoothTransformation
        )
        self.image_label.setPixmap(scaled_pixmap)
        self.image_label.setStyleSheet(
            "border-radius: 12px;"
            "border: 1px solid #dde0e7;"
        )

    def resizeEvent(self, event):
        super().resizeEvent(event)
        self.update_image_display()

    def run_analysis(self):
        """
        使用真正的模型进行青光眼检测
        - 不再显示 demo 文本
        - 不再用颜色区分风险等级
        - 结果全部黑字
        """
        if self.current_image_path is None:
            QMessageBox.information(
                self, "Info", "Please add an image first."
            )
            return

        self.analyze_button.setEnabled(False)
        self.result_label.setText("Analyzing... please wait.")
        self.result_label.setStyleSheet(
            "color: black; background-color: #f8f9fc; "
            "border-radius: 8px; padding: 10px;"
        )
        QApplication.processEvents()

        try:
            result = glaucoma_predict_pipeline(self.current_image_path)

            if "error" in result:
                self.result_label.setText("Error: " + result["error"])
                self.result_label.setStyleSheet(
                    "color: black; background-color: #f8f9fc; "
                    "border-radius: 8px; padding: 10px;"
                )
                return

            text = (
                f"File name: {result['filename']}\n"
                f"Predicted class: {result['predicted_class']}\n"
                f"Probability (Non-Glaucoma): {result['prob_Non-Glaucoma']:.4f}\n"
                f"Probability (Glaucoma): {result['prob_Glaucoma']:.4f}"
            )

            self.result_label.setText(text)
            self.result_label.setStyleSheet(
                "color: black; background-color: #f8f9fc; "
                "border-radius: 8px; padding: 10px;"
            )

        except Exception as e:
            msg = str(e)
            # 把 weights_only 相关的长说明压缩成一句话
            if "Weights only load failed" in msg:
                msg = (
                    "Model weights could not be loaded. "
                    "Please check that '384_unet.pth' and 'efficientnet.pth' "
                    "are in the same folder and are valid checkpoint files."
                )
            self.result_label.setText("Error occurred:\n" + msg)
            self.result_label.setStyleSheet(
                "color: black; background-color: #f8f9fc; "
                "border-radius: 8px; padding: 10px;"
            )
        finally:
            self.analyze_button.setEnabled(True)


class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()

        self.setWindowTitle("Glaucoma Detector")
        self.resize(1000, 650)

        self.stack = QStackedWidget()
        self.welcome_page = WelcomePage()
        self.analyze_page = AnalyzePage()

        self.stack.addWidget(self.welcome_page)
        self.stack.addWidget(self.analyze_page)

        self.setCentralWidget(self.stack)

        self.welcome_page.start_clicked.connect(
            lambda: self.stack.setCurrentWidget(self.analyze_page)
        )


# =======================================================
# main 入口
# =======================================================
def main():
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec_())


if __name__ == "__main__":
    main()
