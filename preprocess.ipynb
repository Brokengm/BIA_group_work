{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T06:37:05.178950Z",
     "iopub.status.busy": "2025-12-10T06:37:05.178615Z",
     "iopub.status.idle": "2025-12-10T06:37:05.196575Z",
     "shell.execute_reply": "2025-12-10T06:37:05.195470Z",
     "shell.execute_reply.started": "2025-12-10T06:37:05.178926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 0. å¯¼å…¥å¿…è¦çš„åº“ ---\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from skimage import exposure, util\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import binary_opening, binary_closing, square\n",
    "import scipy.io as sio # ç”¨äºåŠ è½½ .mat æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T06:37:12.764059Z",
     "iopub.status.busy": "2025-12-10T06:37:12.763692Z",
     "iopub.status.idle": "2025-12-10T06:37:13.043512Z",
     "shell.execute_reply": "2025-12-10T06:37:13.042321Z",
     "shell.execute_reply.started": "2025-12-10T06:37:12.764004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” å¼€å§‹åŠ è½½è®­ç»ƒå¥½çš„ UNet æ¨¡å‹...\n",
      "âš ï¸  weights_only=True åŠ è½½å¤±è´¥ï¼Œå°è¯• weights_only=False\n",
      "âœ… æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼è®¾å¤‡: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 1. UNet ç±»å®šä¹‰ ---\n",
    "class UNet(nn.Module): \n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "        features = init_features\n",
    "        self.encoder1 = self._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = self._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = self._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = self._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = self._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = self._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = self._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = self._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = self._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def _block(self, in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=features,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        logits = self.conv(dec1)\n",
    "        return self.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T06:37:49.454615Z",
     "iopub.status.busy": "2025-12-10T06:37:49.454243Z",
     "iopub.status.idle": "2025-12-10T06:37:49.484197Z",
     "shell.execute_reply": "2025-12-10T06:37:49.483017Z",
     "shell.execute_reply.started": "2025-12-10T06:37:49.454591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. å›¾åƒé¢„å¤„ç†å‡½æ•° ---\n",
    "def get_bounding_box(binary_mask):\n",
    "    \"\"\"\n",
    "    è·å–äºŒå€¼æ©ç ä¸­æœ€å¤§è¿é€šåŒºåŸŸçš„è¾¹ç•Œæ¡†ã€‚\n",
    "    \"\"\"\n",
    "    binary = binary_mask > 0\n",
    "    if not np.any(binary):\n",
    "        height, width = binary_mask.shape\n",
    "        return 0, 0, width, height\n",
    "    \n",
    "    labeled_image = label(binary)\n",
    "    regions = regionprops(labeled_image)\n",
    "    largest_region = max(regions, key=lambda r: r.area)\n",
    "    min_row, min_col, max_row, max_col = largest_region.bbox\n",
    "    return min_col, min_row, max_col - min_col, max_row - min_row\n",
    "\n",
    "\n",
    "def gamma_correct(img, gamma=0.4):\n",
    "    \"\"\"Gamma æ ¡æ­£ï¼šæäº®æš—åŒºç»†èŠ‚\"\"\"\n",
    "    img_float = util.img_as_float(img)\n",
    "    corrected = exposure.adjust_gamma(img_float, gamma=gamma)\n",
    "    return util.img_as_ubyte(corrected)\n",
    "\n",
    "\n",
    "def enhance_contrast_clahe(image, clip_limit=0.02, kernel_size=8):\n",
    "    \"\"\"CLAHE å±€éƒ¨å¯¹æ¯”åº¦å¢å¼º\"\"\"\n",
    "    img_float = util.img_as_float(image)\n",
    "    enhanced = exposure.equalize_adapthist(\n",
    "        img_float,\n",
    "        kernel_size=kernel_size,\n",
    "        clip_limit=clip_limit\n",
    "    )\n",
    "    return util.img_as_ubyte(enhanced)\n",
    "\n",
    "\n",
    "def segment_optic_disc_from_mat(image_path, mat_dir):\n",
    "    \"\"\"\n",
    "    ç›´æ¥ä»ä¸å›¾åƒåŒåçš„ .mat æ–‡ä»¶ä¸­è¯»å–è§†ç›˜æ ‡æ³¨ã€‚\n",
    "    \"\"\"\n",
    "    image_stem = Path(image_path).stem\n",
    "    mat_path = os.path.join(mat_dir, f\"{image_stem}.mat\")\n",
    "    mat_data = sio.loadmat(mat_path)\n",
    "    mask = mat_data['mask']\n",
    "    optic_disc_mask = (mask > 0).astype(np.uint8) * 255\n",
    "    return optic_disc_mask\n",
    "\n",
    "\n",
    "def segment_optic_disc_by_clustering(gray_image):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ KMeans åˆ†å‰²è§†ç›˜åŒºåŸŸã€‚\n",
    "    \"\"\"\n",
    "    pixel_values = gray_image.reshape(-1, 1).astype(np.float32)\n",
    "    kmeans = KMeans(n_clusters=6, n_init=3, random_state=0).fit(pixel_values)\n",
    "    cluster_labels = kmeans.labels_.reshape(gray_image.shape)\n",
    "    cluster_brightness_means = []\n",
    "    for cluster_id in range(6):\n",
    "        pixels_in_cluster = pixel_values[cluster_labels.ravel() == cluster_id]\n",
    "        mean_brightness = pixels_in_cluster.mean() if len(pixels_in_cluster) > 0 else -np.inf\n",
    "        cluster_brightness_means.append(mean_brightness)\n",
    "    brightest_cluster_ids = np.argsort(cluster_brightness_means)[-2:]\n",
    "    optic_disc_mask = np.isin(cluster_labels, brightest_cluster_ids)\n",
    "    return (optic_disc_mask * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def segment_optic_disc_with_unet(image_path, unet_model):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ UNet æ¨¡å‹åˆ†å‰²è§†ç›˜åŒºåŸŸã€‚\n",
    "    \"\"\"\n",
    "    pil_image = Image.open(image_path).convert('RGB')\n",
    "    original_img = np.array(pil_image)\n",
    "    \n",
    "    img_tensor = util.img_as_float(original_img)\n",
    "    img_tensor = resize(img_tensor, (224, 224), anti_aliasing=True)\n",
    "    img_tensor = torch.FloatTensor(img_tensor).permute(2, 0, 1).unsqueeze(0)  \n",
    "    # ä»æ¨¡å‹å¯¹è±¡è·å–å…¶æ‰€åœ¨çš„è®¾å¤‡\n",
    "    model_device = next(unet_model.parameters()).device\n",
    "    img_tensor = img_tensor.to(model_device) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = unet_model(img_tensor)\n",
    "        mask_pred = output.squeeze().cpu().numpy()\n",
    "        mask_pred = (mask_pred > 0.5).astype(np.uint8) \n",
    "    \n",
    "    mask_pred = resize(mask_pred, original_img.shape[:2], anti_aliasing=False, order=0)\n",
    "    mask_pred = (mask_pred > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    return mask_pred\n",
    "\n",
    "\n",
    "def crop_fundus_region(original_image, threshold=10):\n",
    "    \"\"\"\n",
    "    è£å‰ªçœ¼åº•æœ‰æ•ˆåŒºåŸŸï¼ˆå»é™¤é»‘è¾¹/æ— æ•ˆåŒºåŸŸï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    if original_image.ndim == 2:\n",
    "        original_image = np.stack([original_image] * 3, axis=-1)\n",
    "    \n",
    "    gray_image = util.img_as_ubyte(rgb2gray(original_image))\n",
    "    \n",
    "    if np.max(gray_image) <= threshold:\n",
    "        foreground_mask = np.ones_like(gray_image, dtype=np.uint8) * 255\n",
    "    else:\n",
    "        foreground_mask = (gray_image > threshold).astype(np.uint8) * 255\n",
    "\n",
    "    min_col, min_row, width, height = get_bounding_box(foreground_mask)\n",
    "    \n",
    "    margin = min(100, width // 4, height // 4)\n",
    "    col_start = max(0, min_col + margin)\n",
    "    row_start = max(0, min_row + margin)\n",
    "    col_end = min(original_image.shape[1], min_col + width - margin)\n",
    "    row_end = min(original_image.shape[0], min_row + height - margin)\n",
    "    \n",
    "    if col_end <= col_start or row_end <= row_start:\n",
    "        col_start, row_start, col_end, row_end = 0, 0, original_image.shape[1], original_image.shape[0]\n",
    "\n",
    "    cropped_rgb = original_image[row_start:row_end, col_start:col_end]\n",
    "    cropped_gray = gray_image[row_start:row_end, col_start:col_end]\n",
    "    return cropped_rgb, cropped_gray\n",
    "\n",
    "\n",
    "def refine_mask_with_morphology(binary_mask):\n",
    "    \"\"\"é€šè¿‡å½¢æ€å­¦æ“ä½œï¼ˆå¼€+é—­ï¼‰ç²¾ä¿®äºŒå€¼æ©ç \"\"\"\n",
    "    binary = binary_mask > 0\n",
    "    opened = binary_opening(binary, footprint=square(5))\n",
    "    closed = binary_closing(opened, footprint=square(5))\n",
    "    return (closed * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def crop_optic_disc_region(rgb_image, optic_disc_mask):\n",
    "    \"\"\"\n",
    "    åŸºäºè§†ç›˜æ©ç è£å‰ªå‡ºåŒ…å«è§†ç›˜çš„å±€éƒ¨åŒºåŸŸï¼Œå¹¶ç¼©æ”¾åˆ° 224x224ã€‚\n",
    "    \"\"\"\n",
    "    min_col, min_row, width, height = get_bounding_box(optic_disc_mask)\n",
    "    \n",
    "    scale_x = rgb_image.shape[1] / optic_disc_mask.shape[1]\n",
    "    scale_y = rgb_image.shape[0] / optic_disc_mask.shape[0]\n",
    "    min_col = int(min_col * scale_x)\n",
    "    width = int(width * scale_x)\n",
    "    min_row = int(min_row * scale_y)\n",
    "    height = int(height * scale_y)\n",
    "    \n",
    "    center_x = min_col + width // 2\n",
    "    center_y = min_row + height // 2\n",
    "    \n",
    "    half_crop_size = 250  # è£å‰ª 500x500 åŒºåŸŸ\n",
    "    col_start = max(0, center_x - half_crop_size)\n",
    "    row_start = max(0, center_y - half_crop_size)\n",
    "    col_end = min(rgb_image.shape[1], center_x + half_crop_size)\n",
    "    row_end = min(rgb_image.shape[0], center_y + half_crop_size)\n",
    "    \n",
    "    if col_end <= col_start or row_end <= row_start:\n",
    "        col_start, row_start = 0, 0\n",
    "        col_end = min(rgb_image.shape[1], 500)\n",
    "        row_end = min(rgb_image.shape[0], 500)\n",
    "    \n",
    "    optic_disc_crop = rgb_image[row_start:row_end, col_start:col_end]\n",
    "    final_optic_disc_crop = util.img_as_ubyte(\n",
    "        resize(optic_disc_crop, (224, 224), anti_aliasing=True)\n",
    "    )\n",
    "    return final_optic_disc_crop\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_fundus_image(image_path, mat_dir=None, method='unet', unet_model=None, debug=False):\n",
    "    \"\"\"\n",
    "    ç«¯åˆ°ç«¯é¢„å¤„ç†ï¼šè¾“å…¥åŸå§‹çœ¼åº•å›¾ï¼Œè¾“å‡º 224x224 è§†ç›˜è£å‰ªå›¾ã€‚\n",
    "    \"\"\"\n",
    "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = np.array(pil_image)\n",
    "    \n",
    "    cropped_rgb, cropped_gray = crop_fundus_region(original_image, threshold=10)\n",
    "    \n",
    "    enhanced_gray = gamma_correct(cropped_gray, gamma=0.4)\n",
    "    enhanced_gray = enhance_contrast_clahe(enhanced_gray)\n",
    "    \n",
    "    if method == 'clustering':\n",
    "        optic_disc_mask = segment_optic_disc_by_clustering(enhanced_gray)\n",
    "    elif method == 'unet':\n",
    "        if unet_model is None:\n",
    "            raise ValueError(\"method='unet', must provide model\")\n",
    "        optic_disc_mask = segment_optic_disc_with_unet(image_path, unet_model)\n",
    "    elif method == 'mat':\n",
    "        if mat_dir is None:\n",
    "            raise ValueError(\"method='mat', must provide mat file path\")\n",
    "        optic_disc_mask = segment_optic_disc_with_unet(image_path, unet_model)\n",
    "        optic_disc_mask = segment_optic_disc_from_mat(image_path, mat_dir)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown mehtod: {method}\")\n",
    "    \n",
    "    refined_mask = refine_mask_with_morphology(optic_disc_mask)\n",
    "    \n",
    "    enhanced_rgb = enhance_contrast_clahe(cropped_rgb)\n",
    "    final_crop = crop_optic_disc_region(enhanced_rgb, refined_mask)\n",
    "    \n",
    "    if debug:\n",
    "        return {\n",
    "            \"original_image\": original_image,\n",
    "            \"cropped_fundus\": cropped_rgb,\n",
    "            \"optic_disc_mask\": refined_mask,\n",
    "            \"final_optic_disc_crop\": final_crop\n",
    "        }\n",
    "    else:\n",
    "        return final_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. å®šä¹‰åŠ è½½æ¨¡å‹çš„å‡½æ•° ---\n",
    "def load_model(model_class, model_save_path, in_channels=3, out_channels=1, device=None):\n",
    "    \"\"\"\n",
    "    åŠ è½½è®­ç»ƒå¥½çš„ PyTorch æ¨¡å‹ã€‚\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"use devide: {device}\")\n",
    "\n",
    "    model = model_class(in_channels=in_channels, out_channels=out_channels).to(device)\n",
    "    model.eval()  \n",
    "\n",
    "    try:\n",
    "        with torch.serialization.safe_globals([np.dtype, np.core.multiarray.scalar]):\n",
    "            checkpoint = torch.load(model_save_path, map_location=device, weights_only=True)\n",
    "    except Exception as e:\n",
    "        print(f\"weights_only=True failed: {e}\")\n",
    "        print(\"try weights_only=False...\")\n",
    "        checkpoint = torch.load(model_save_path, map_location=device, weights_only=False)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    print(f\"model loaded\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. æ‰¹é‡é¢„å¤„ç†å‡½æ•° ---\n",
    "def batch_preprocess_images(\n",
    "    input_directory,\n",
    "    output_directory,\n",
    "    method='unet',\n",
    "    unet_model=None,\n",
    "    image_extension=\"*.jpg\",\n",
    "    save_quality=95\n",
    "):\n",
    "    \"\"\"\n",
    "    æ‰¹é‡é¢„å¤„ç†çœ¼åº•å›¾åƒï¼ˆæ”¯æŒ UNetã€èšç±»ã€MAT ç­‰æ–¹æ³•ï¼‰\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    print(f\"output directory exists: {output_directory}\")\n",
    "\n",
    "    image_paths = sorted(glob(os.path.join(input_directory, image_extension)))\n",
    "    if not image_paths:\n",
    "        print(f\"No any {image_extension} file in {input_directory}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    for idx, image_path in enumerate(image_paths, 1):\n",
    "        filename = Path(image_path).name\n",
    "        print(f\" ({idx}/{len(image_paths)}) now: {filename}\")\n",
    "\n",
    "        final_crop = preprocess_fundus_image(\n",
    "            image_path=image_path,\n",
    "            method=method,\n",
    "            unet_model=unet_model if method == 'unet' else None)\n",
    "\n",
    "        output_filename = f\"{Path(image_path).stem}_preprocessed.jpg\"\n",
    "        output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "        img_to_save = Image.fromarray(final_crop)\n",
    "        img_to_save.save(output_path, quality=save_quality)\n",
    "\n",
    "        print(f\" saved in : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\n",
    "    model_class=UNet,\n",
    "    model_save_path=\"/kaggle/input/best-unet/pytorch/default/1/best_unet_optic_disc.pth\",\n",
    "    in_channels=3,\n",
    "    out_channels=1\n",
    ")\n",
    "\n",
    "batch_preprocess_images(\n",
    "    input_directory=\"/kaggle/input/glaucoma-detection/ORIGA/ORIGA/Images/\",\n",
    "    output_directory=\"/kaggle/working/preprocessed_optic_discs_unet/\",\n",
    "    method='unet',\n",
    "    unet_model=model,\n",
    "    image_extension=\"*.jpg\",\n",
    "    save_quality=95\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 857145,
     "sourceId": 3938763,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 529141,
     "modelInstanceId": 514493,
     "sourceId": 678372,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
